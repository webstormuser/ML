{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c45fa6a6",
   "metadata": {},
   "source": [
    "# 1. What is the definition of a target function? In the sense of a real-life example, express the target function. How is a target function's fitness assessed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9531fae",
   "metadata": {},
   "source": [
    "ANS : A target function is a mathematical function used in machine learning and optimization algorithms to model and predict the relationship between input variables and output variables. It is the function that the machine learning algorithm tries to approximate or optimize during training.\n",
    "\n",
    "In a real-life example, suppose you want to predict the housing prices in a city based on factors such as the number of bedrooms, the size of the house, the location, and the age of the property. The target function in this case would be a mathematical function that takes these input variables and predicts the output variable, i.e., the price of the house.\n",
    "\n",
    "The fitness of a target function is typically assessed using a performance metric, which measures how well the function performs in predicting the output variable based on the input variables. For example, in the housing price example, a common performance metric is the mean squared error, which measures the average difference between the predicted prices and the actual prices of a set of houses. The fitness of the target function is evaluated by minimizing this performance metric, i.e., by finding the parameters of the function that minimize the mean squared error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62ffb8d",
   "metadata": {},
   "source": [
    "# 2. What are predictive models, and how do they work? What are descriptive types, and how do you use them? Examples of both types of models should be provided. Distinguish between these two forms of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0de241",
   "metadata": {},
   "source": [
    "ANS :\n",
    "* Predictive models and descriptive models are two types of models used in data analysis, machine learning, and statistics. Both types of models serve different purposes, and their applications depend on the specific goals of the project.\n",
    "\n",
    "* Predictive models aim to predict the outcome of a future event based on historical data. These models are trained using labeled data that contains both the input variables and the corresponding output variable. Once the model is trained, it can be used to make predictions on new, unseen data. Predictive models can be categorized as either regression or classification models. Regression models predict a continuous output variable, while classification models predict a categorical output variable.\n",
    "\n",
    "* An example of a predictive model is a linear regression model that predicts a house's sale price based on its square footage, number of bedrooms, and other features. Another example is a classification model that predicts whether a customer will churn or not based on their purchase history and demographics.\n",
    "\n",
    "* On the other hand, descriptive models aim to describe and summarize the data without making predictions. These models are used to understand the relationships between variables and identify patterns in the data. Descriptive models can be simple, such as calculating summary statistics, or more complex, such as clustering or association rule mining.\n",
    "\n",
    "* An example of a descriptive model is a heatmap that shows the correlation between different features in a dataset. Another example is a decision tree that shows the relationships between different factors affecting a business's success.\n",
    "\n",
    "* In summary, predictive models are used to predict the outcome of future events, while descriptive models are used to describe and understand the relationships between variables in the data. Both types of models are important in data analysis and serve different purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf46299",
   "metadata": {},
   "source": [
    "# 3. Describe the method of assessing a classification model's efficiency in detail. Describe the various measurement parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4483f487",
   "metadata": {},
   "source": [
    "ANS :\n",
    "   * Assessing the efficiency of a classification model involves evaluating how well it can predict the class labels of new, unseen data based on the patterns it has learned from the training data. There are several methods and metrics used for evaluating classification models, and the choice of the appropriate metric depends on the specific problem and the goal of the model. Here are some of the most common evaluation methods and metrics:\n",
    "\n",
    "   *  Confusion matrix: A confusion matrix is a table that shows the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) produced by the model. The rows represent the actual class labels, and the columns represent the predicted class labels. This matrix can be used to compute various evaluation metrics.\n",
    "\n",
    "   * Accuracy: Accuracy is the most commonly used metric for evaluating classification models. It is defined as the ratio of the number of correctly classified instances (TP + TN) to the total number of instances in the test set. While accuracy is a useful metric for balanced datasets, it may not be informative for imbalanced datasets where the class distribution is skewed.\n",
    "\n",
    "   * Precision: Precision is the proportion of true positives among the instances that the model predicted as positive (TP / (TP + FP)). Precision measures how well the model identifies positive instances, and it is particularly useful when the cost of a false positive is high.\n",
    "\n",
    "   * Recall: Recall is the proportion of true positives among all the instances that belong to the positive class (TP / (TP + FN)). Recall measures how well the model captures positive instances, and it is particularly useful when the cost of a false negative is high.\n",
    "\n",
    "   * F1-score: The F1-score is the harmonic mean of precision and recall. It is a measure of the overall performance of the model, and it is particularly useful when the class distribution is imbalanced.\n",
    "\n",
    "   * ROC curve: The receiver operating characteristic (ROC) curve is a plot of the true positive rate (TPR) against the false positive rate (FPR) for different threshold values of the model's prediction score. The area under the curve (AUC) is a useful metric for evaluating the performance of a binary classifier, and it is particularly useful when the class distribution is imbalanced.\n",
    "\n",
    "   * Confusion matrix-based metrics: Several metrics can be computed from the confusion matrix, such as sensitivity (recall), specificity (proportion of true negatives among all negative instances), and FPR (proportion of false positives among all negative instances). These metrics are useful for understanding the model's performance on different classes.\n",
    "\n",
    "* In conclusion, the choice of evaluation method and metric depends on the specific classification problem and the goal of the model. A combination of metrics can provide a more comprehensive understanding of the model's strengths and weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbd8388",
   "metadata": {},
   "source": [
    "# 4.  i. In the sense of machine learning models, what is underfitting? What is the most common reason for underfitting?ii. What does it mean to overfit? When is it going to happen?iii. In the sense of model fitting, explain the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58890878",
   "metadata": {},
   "source": [
    "ANS :\n",
    "\n",
    "i. Underfitting occurs when a machine learning model is too simple to capture the complexity of the data, resulting in poor performance on both the training and testing sets. The most common reason for underfitting is using a model that is not complex enough to capture the patterns in the data, such as using a linear model to fit a non-linear relationship between the features and target variable.\n",
    "\n",
    "ii. Overfitting occurs when a machine learning model is too complex and captures noise or random fluctuations in the training data, leading to excellent performance on the training set but poor performance on the testing set. Overfitting usually happens when a model has too many parameters or is too flexible, which allows it to fit the training data too closely.\n",
    "\n",
    "iii. The bias-variance trade-off is a fundamental concept in machine learning that explains the trade-off between model bias and variance. Bias refers to the difference between the expected (or average) predictions of the model and the true values, while variance refers to the variability of the predictions for different training sets. A model with high bias tends to underfit the data, while a model with high variance tends to overfit the data. The goal is to find the sweet spot between bias and variance, where the model generalizes well to unseen data. Increasing the complexity of the model can reduce bias but increase variance, while decreasing the complexity of the model can reduce variance but increase bias. Therefore, the key to finding the optimal trade-off is to balance bias and variance by selecting an appropriate model complexity or regularization technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a921bfb6",
   "metadata": {},
   "source": [
    "# 5. Is it possible to boost the efficiency of a learning model? If so, please clarify how."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da42634e",
   "metadata": {},
   "source": [
    "ANS : \n",
    "    Yes, it is possible to boost the efficiency of a learning model using several techniques. Here are some of the most effective ways:\n",
    "\n",
    "    * Feature engineering: Feature engineering involves selecting or transforming the input variables of a model to improve its performance. It can be done manually or using automatic feature selection techniques.\n",
    "\n",
    "    * Hyperparameter tuning: Hyperparameters are parameters that are set before the training of a model, such as the learning rate or the number of layers in a neural network. Tuning these hyperparameters can significantly improve the performance of a model.\n",
    "\n",
    "    * Regularization: Regularization is a technique used to prevent overfitting in a model. It involves adding a penalty term to the loss function to discourage the model from fitting too closely to the training data.\n",
    "\n",
    "    * Ensemble learning: Ensemble learning involves combining the predictions of multiple models to improve their overall performance. It can be done using techniques such as bagging, boosting, and stacking.\n",
    "\n",
    "    * Transfer learning: Transfer learning involves using a pre-trained model as a starting point for a new model. This can significantly reduce the amount of time and data required to train a new model.\n",
    "\n",
    "    * Batch normalization: Batch normalization is a technique used to normalize the inputs of a model in order to reduce the internal covariate shift. This can improve the speed and stability of training.\n",
    "\n",
    "    * Early stopping: Early stopping involves stopping the training of a model before it has converged completely. This can prevent overfitting and improve the generalization performance of the model.\n",
    "\n",
    "    * Data augmentation: Data augmentation involves generating new training examples by applying random transformations to the existing data. This can increase the amount of training data available to the model and improve its performance.\n",
    "\n",
    "By implementing one or more of these techniques, it is possible to significantly boost the efficiency of a learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc0554d",
   "metadata": {},
   "source": [
    "# 6. How would you rate an unsupervised learning model's success? What are the most common success indicators for an unsupervised learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397f960b",
   "metadata": {},
   "source": [
    "ANS :\n",
    "    Evaluating the success of an unsupervised learning model is not as straightforward as in supervised learning, where we have a predefined set of labels to measure the model's accuracy against. In unsupervised learning, there are no ground truth labels to evaluate the model's performance against. However, there are several methods that can be used to evaluate the success of unsupervised learning models, including:\n",
    "\n",
    "   1)  Clustering Metrics: Clustering is one of the most common unsupervised learning tasks, where the goal is to group similar data points together. Clustering metrics such as silhouette score, Davies-Bouldin Index, and Calinski-Harabasz Index can be used to evaluate the quality of the clusters formed by the model.\n",
    "\n",
    "   2) Visualization: Visualization techniques such as t-SNE, PCA, and UMAP can be used to visualize the data in lower dimensions and inspect the clusters formed by the model visually.\n",
    "\n",
    "   3) Reconstruction error: In unsupervised learning tasks such as autoencoders, where the goal is to learn a lower-dimensional representation of the data, the reconstruction error can be used as a measure of the model's success.\n",
    "\n",
    "   4) Outlier detection: Unsupervised learning models can also be used for outlier detection. The success of the model can be evaluated by the ability to detect known outliers in the dataset.\n",
    "\n",
    "   5) Novelty detection: Unsupervised learning models can also be used to detect novel or anomalous data points that do not fit within the learned representation of the data.\n",
    "\n",
    "In summary, the success of an unsupervised learning model can be evaluated using a combination of clustering metrics, visualization techniques, reconstruction error, outlier detection, and novelty detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1528525e",
   "metadata": {},
   "source": [
    "# 7. Is it possible to use a classification model for numerical data or a regression model for categorical data with a classification model? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acb3d62",
   "metadata": {},
   "source": [
    "ANS :No, it is not recommended to use a classification model for numerical data or a regression model for categorical data.\n",
    "\n",
    "Classification models are designed to predict discrete categorical values, while regression models are designed to predict continuous numerical values. Using the wrong type of model can result in poor performance and inaccurate predictions.\n",
    "\n",
    "For numerical data, regression models such as linear regression, polynomial regression, and decision trees are commonly used. These models aim to predict the value of a dependent variable based on one or more independent variables.\n",
    "\n",
    "For categorical data, classification models such as logistic regression, decision trees, and support vector machines (SVMs) are typically used. These models aim to predict the class or category of a dependent variable based on one or more independent variables.\n",
    "\n",
    "If the data is incorrectly labeled as numerical or categorical, the choice of the model should still be based on the type of variable being predicted. If the data is not clearly defined as either numerical or categorical, exploratory data analysis should be performed to determine the appropriate model type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc8d864",
   "metadata": {},
   "source": [
    "# 8. Describe the predictive modeling method for numerical values. What distinguishes it from categorical predictive modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3213c470",
   "metadata": {},
   "source": [
    "ANS :\n",
    "    \n",
    "   * Predictive modeling for numerical values is a statistical technique used to build models that predict continuous numerical outcomes. It involves using data to identify patterns and relationships between variables, and then using these patterns to make predictions about future outcomes. Some common methods for numerical predictive modeling include linear regression, decision trees, and neural networks.\n",
    "\n",
    "    * One of the main distinguishing factors between numerical and categorical predictive modeling is the type of outcome being predicted. Categorical predictive modeling involves predicting discrete outcomes, such as whether a customer will buy a product or not, or which category a particular product falls into. Numerical predictive modeling, on the other hand, is used to predict continuous values such as the temperature, stock prices, or customer lifetime value.\n",
    "\n",
    "    * Another difference is in the evaluation metrics used for measuring the accuracy of the model. In categorical predictive modeling, metrics such as accuracy, precision, and recall are commonly used, while in numerical predictive modeling, metrics such as mean squared error (MSE) or mean absolute error (MAE) are often used.\n",
    "\n",
    "    * Additionally, the type of algorithms used for numerical predictive modeling may be different from those used for categorical predictive modeling. For example, decision trees and neural networks can be used for both types of modeling, but linear regression is typically used only for numerical predictive modeling.\n",
    "\n",
    "In summary, while both numerical and categorical predictive modeling share some similarities, such as the need for training data and algorithms, they differ in terms of the outcome being predicted, evaluation metrics, and the algorithms used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75057ee",
   "metadata": {},
   "source": [
    "# 9. The following data were collected when using a classification model to predict the malignancy of a group of patients' tumors:\n",
    "         i. Accurate estimates – 15 cancerous, 75 benign\n",
    "         ii. Wrong predictions – 3 cancerous, 7 benign\n",
    "                Determine the model's error rate, Kappa value, sensitivity, precision, and F-measure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0c64fc",
   "metadata": {},
   "source": [
    "ANS : \n",
    "    \n",
    "    * Kappa Value :The kappa statistic is a metric used in classification tasks to evaluate the performance of a machine learning algorithm. It measures the agreement between the predicted and actual labels of a dataset, taking into account the possibility of the agreement occurring by chance. The kappa value ranges from -1 to 1, with higher values indicating better performance.\n",
    "        In other words, kappa is a statistical measure of the degree of similarity between the predicted and actual labels, adjusted for chance agreement. It is often used when evaluating the performance of a classification algorithm, especially in cases where the classes are imbalanced or the dataset has a large number of uninformative instances.\n",
    "        Kappa value can be calculated using the following formula:\n",
    "\n",
    "kappa = (observed agreement - expected agreement) / (1 - expected agreement)\n",
    "\n",
    "To compute these metrics, we first need to define the following terms:\n",
    "\n",
    "    True Positive (TP): Number of cancerous tumors correctly predicted as cancerous.\n",
    "    False Positive (FP): Number of benign tumors incorrectly predicted as cancerous.\n",
    "    True Negative (TN): Number of benign tumors correctly predicted as benign.\n",
    "    False Negative (FN): Number of cancerous tumors incorrectly predicted as benign.\n",
    "        \n",
    "        TP==15 TN==75 FP==3 FN==7\n",
    "        \n",
    "*  Error Rate: This is the proportion of incorrect predictions made by the model.\n",
    "\n",
    "    *  Error Rate = (FP + FN) / (TP + FP + TN + FN)\n",
    "                  = (7 + 3) / (15 + 7 + 75 + 3)\n",
    "                  = 0.073\n",
    "            \n",
    "* Kappa Value: This is a measure of how well the model agrees with the actual classification, taking into account the agreement that would be expected by chance. A kappa value of 1 indicates perfect agreement, while a value of 0 indicates agreement no better than chance.\n",
    "\n",
    "    Let's compute the kappa value using the formula:\n",
    "\n",
    "    Kappa = (p_o - p_e) / (1 - p_e)\n",
    "\n",
    "    where p_o is the observed agreement and p_e is the expected agreement.\n",
    "\n",
    "    Observed Agreement (p_o) = (TP + TN) / (TP + FP + TN + FN)\n",
    "    = (15 + 75) / (15 + 7 + 75 + 3)\n",
    "    = 0.90\n",
    "\n",
    "    Expected Agreement (p_e) = [(TP + FP) * (TP + FN) + (FN + TN) * (FP + TN)] / (TP + FP + TN + FN)^2\n",
    "                             = [(15 + 7) * (15 + 3) + (3 + 75) * (7 + 75)] / (15 + 7 + 75 + 3)^2\n",
    "                             = 0.792\n",
    "                        Kappa = (p_o - p_e) / (1 - p_e)\n",
    "                              = (0.90 - 0.792) / (1 - 0.792)\n",
    "                              = 0.476\n",
    "            Therefore, the kappa value is 0.476.\n",
    "            \n",
    "* Sensitivity: This is the proportion of actual cancerous tumors that are correctly predicted as cancerous.\n",
    "\n",
    "    Sensitivity = TP / (TP + FN)\n",
    "    = 15 / (15 + 3)\n",
    "    = 0.833\n",
    "\n",
    "    Therefore, the sensitivity is 0.833 or 83.3%.\n",
    "\n",
    "* Precision: This is the proportion of predicted cancerous tumors that are actually cancerous.\n",
    "\n",
    "    Precision = TP / (TP + FP)\n",
    "    = 15 / (15 + 7)\n",
    "    = 0.682\n",
    "\n",
    "    Therefore, the precision is 0.682 or 68.2%.\n",
    "\n",
    "* F-Measure: This is a weighted average of precision and sensitivity, where the F1 score gives equal weight to both measures.\n",
    "\n",
    "    F1 Score = 2 * (Precision * Sensitivity) / (Precision + Sensitivity)\n",
    "            = 2 * (0.682 * 0.833) / (0.682 + 0.833)\n",
    "            = 0.750\n",
    "\n",
    "    Therefore, the F1 score is 0.750.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a330d0",
   "metadata": {},
   "source": [
    "# 10. Make quick notes on:\n",
    "         1. The process of holding out\n",
    "         2. Cross-validation by tenfold\n",
    "         3. Adjusting the parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a69046",
   "metadata": {},
   "source": [
    "ANS :\n",
    "     *  The process of holding out :\n",
    "     \n",
    "     The process of \"holding out\" can refer to a few different things depending on the context. Here are a few possible explanations:\n",
    "\n",
    "    In statistics, \"holding out\" refers to a technique used in model validation. It involves splitting a dataset into two parts: a training set and a validation set. The model is trained on the training set, and then evaluated on the validation set to see how well it generalizes to new data. The validation set is \"held out\" from the training process to prevent overfitting.\n",
    "\n",
    "       In negotiation, \"holding out\" refers to the act of refusing to make a deal or compromise until certain conditions are met. For example, if a union is negotiating with management for better wages, they may \"hold out\" until their demands are met.\n",
    "\n",
    "       In sports, \"holding out\" refers to a player refusing to participate in team activities (such as training camp or games) until their contract demands are met. This is typically done as a negotiating tactic to try to get a better contract.\n",
    "\n",
    "    Overall, the term \"holding out\" usually refers to a situation where someone is refusing to give in or compromise until certain conditions are met.\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    " * Cross Validation by Tenfold:\n",
    " \n",
    "         Cross-validation is a technique used in machine learning to assess the performance of a model by testing it on data that it has not been trained on. Tenfold cross-validation is a specific type of cross-validation where the data is divided into ten equally-sized parts or \"folds\". The model is then trained on nine of these folds and tested on the remaining one. This process is repeated ten times, with each fold serving as the test set once.\n",
    "\n",
    "        The results of each of the ten tests are then averaged to obtain a single performance metric for the model. Tenfold cross-validation is a popular choice for model evaluation because it provides a good balance between bias and variance. By using multiple folds, it reduces the risk of overfitting, while still providing a reliable estimate of the model's performance.\n",
    "\n",
    "        In summary, tenfold cross-validation is a useful technique for evaluating the performance of machine learning models. It involves dividing the data into ten equally-sized parts, training the model on nine of these parts, and testing it on the remaining part. This process is repeated ten times, and the results are averaged to obtain a reliable estimate of the model's performance.\n",
    "        \n",
    "        \n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "* Adjusting the parameters :\n",
    "\n",
    "        Adjusting the parameters of a system or a model can have a significant impact on its performance and behavior. Depending on the specific system or model, different parameters may need to be adjusted to achieve the desired outcome.\n",
    "\n",
    "        For example, in machine learning, the parameters of a model can be adjusted to improve its accuracy and reduce errors. These parameters might include learning rate, regularization, number of hidden layers, number of neurons per layer, and activation functions. By adjusting these parameters, a machine learning model can be trained to perform better on specific tasks.\n",
    "\n",
    "        In engineering, the parameters of a system can be adjusted to optimize its performance. This might include adjusting the parameters of a control system to improve stability and response time, or adjusting the parameters of a manufacturing process to improve efficiency and quality.\n",
    "\n",
    "        When adjusting parameters, it is important to have a clear understanding of the system or model being optimized, as well as the desired outcome. It may be necessary to experiment with different parameter values and observe the resulting behavior in order to find the optimal settings.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533c9d19",
   "metadata": {},
   "source": [
    "# 11. Define the following terms: \n",
    "         1. Purity vs. Silhouette width\n",
    "         2. Boosting vs. Bagging\n",
    "         3. The eager learner vs. the lazy learner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf4e0c",
   "metadata": {},
   "source": [
    "ANS :\n",
    "    1. Purity vs. Silhouette width:\n",
    "        \n",
    "        Purity and silhouette width are both metrics commonly used to evaluate the quality of clusterings in unsupervised machine learning.\n",
    "    Purity measures the proportion of data points in a cluster that belong to the same true class or label. It ranges from 0 to 1, with higher values indicating better clustering performance in terms of accurately grouping similar data points together.\n",
    "    On the other hand, silhouette width measures the similarity of a data point to its own cluster compared to other clusters. It ranges from -1 to 1, with higher values indicating better clustering performance in terms of well-separated clusters.\n",
    "    While purity and silhouette width are both important metrics for evaluating clustering performance, they focus on different aspects of clustering. Purity is more concerned with the accuracy of the clustering in terms of grouping similar data points together, while silhouette width is more concerned with the separation of clusters and how distinct they are from one another.\n",
    "    In practice, which metric to prioritize may depend on the specific problem and goals of the clustering analysis. For example, if the goal is to identify groups of similar items for marketing segmentation, purity may be more important. However, if the goal is to identify distinct groups for anomaly detection, silhouette width may be more important.\n",
    "    \n",
    "    \n",
    "======================================================================================================================\n",
    "\n",
    "  2. Boosting vs. Bagging:\n",
    "          \n",
    "          Boosting and bagging are two popular ensemble methods used in machine learning for improving the performance of models by combining the outputs of multiple base models. However, they differ in how they generate the ensemble.\n",
    "\n",
    "    Bagging, short for bootstrap aggregating, involves creating multiple bootstrap samples from the original dataset, training a base model on each bootstrap sample, and combining the outputs of these models through averaging or majority voting. Bagging is effective in reducing overfitting and improving the stability of the model, especially for high-variance models such as decision trees.\n",
    "\n",
    "    Boosting, on the other hand, is an iterative approach that involves sequentially training base models, where each subsequent model focuses on correcting the errors of the previous model. In boosting, each base model is trained on a weighted version of the original dataset, with more weight given to the misclassified instances. The final prediction is a weighted combination of the predictions of all base models. Boosting is effective in reducing bias and improving the accuracy of the model, especially for high-bias models such as linear models.\n",
    "\n",
    "    In summary, bagging focuses on reducing variance by averaging multiple models, while boosting focuses on reducing bias by iteratively improving the accuracy of the model.\n",
    "    \n",
    "    \n",
    "======================================================================================================================\n",
    "\n",
    "3. The eager learner vs. the lazy learner\n",
    "    \n",
    "        * Lazy learner:\n",
    "\n",
    "            * Just store Data set without learning from it\n",
    "\n",
    "            * Start classifying data when it receive Test data\n",
    "\n",
    "            * So it takes less time learning and more time classifying data\n",
    "\n",
    "        * Eager learner:\n",
    "\n",
    "            * When it receive data set it starts classifying (learning)\n",
    "\n",
    "            \n",
    "            * Then it does not wait for test data to learn\n",
    "\n",
    "            * So it takes long time learning and less time classifying data\n",
    "\n",
    "    Hint : In supervised learning\n",
    "\n",
    "        Some examples are :\n",
    "\n",
    "        Lazy : K - Nearest Neighbour, Case - Based Reasoning\n",
    "\n",
    "        Eager : Decision Tree, Naive Bayes, Artificial Neural Networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe7264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
