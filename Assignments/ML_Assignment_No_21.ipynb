{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What is the estimated depth of a Decision Tree trained (unrestricted) on a one million instance training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS:The estimated depth of a Decision Tree trained on a one million instance training set would depend on various factors including the complexity of the data, the features used, the depth regularization applied, and the algorithm's stopping criteria. \n",
    "\n",
    "Decision Trees can grow to depths where they perfectly fit the training data (reaching a depth of 1 less than the number of instances in the leaf nodes), but this can lead to overfitting, meaning the tree captures noise in the data and doesn't generalize well to new, unseen data. To prevent overfitting, techniques like pruning and limiting the maximum depth are often used.\n",
    "\n",
    "In practice, without any depth restrictions, a Decision Tree could potentially reach a depth close to the number of instances in the training set (one million in this case). However, this might result in overfitting, and it's common to apply techniques like cross-validation to determine an appropriate depth that balances model complexity and generalization.\n",
    "\n",
    "Keep in mind that there's no fixed rule for determining the exact depth, as it heavily depends on the characteristics of the dataset and the goals of the modeling task. It's advisable to experiment with different depths and evaluate the model's performance on validation or test data to find the best trade-off between bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Is the Gini impurity of a node usually lower or higher than that of its parent? Is it always lower/greater, or is it usually lower/greater?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS:The Gini impurity of a node in a Decision Tree is usually lower than or equal to that of its parent. The Gini impurity is a measure of the level of impurity or uncertainty in a dataset, and Decision Trees aim to reduce impurity as they split nodes during their construction.\n",
    "\n",
    "When a Decision Tree creates a new split, it's looking to separate the data into subsets that are more homogeneous in terms of their target labels. This means that, in most cases, the Gini impurity of the child nodes (resulting from the split) will be lower than the Gini impurity of the parent node. The goal is to create splits that lead to purer subsets, making classification decisions more accurate.\n",
    "\n",
    "However, it's important to note that there might be cases where a split increases the Gini impurity of a node slightly due to the distribution of the class labels. These situations can arise when a split doesn't perfectly separate the classes, but the increase in impurity is generally small. The overall goal of a Decision Tree is to reduce impurity and create a more predictive model, so even if the Gini impurity increases slightly for a particular split, subsequent splits will often try to compensate for that by further reducing impurity in other parts of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explain if its a good idea to reduce max depth if a Decision Tree is overfitting the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS:Yes, reducing the maximum depth of a Decision Tree can be a good idea if the tree is overfitting the training set. Overfitting occurs when a model, such as a Decision Tree, learns to capture noise and random fluctuations in the training data, rather than capturing the underlying patterns that generalize well to new, unseen data. Reducing the maximum depth is a common strategy to mitigate overfitting in Decision Trees.\n",
    "\n",
    "Here's why reducing the maximum depth can help:\n",
    "\n",
    "1. **Simplification of the Model**: A Decision Tree with a deeper depth can become highly complex, leading to an intricate and detailed representation of the training data. This can cause the tree to memorize noise rather than learn meaningful patterns. By reducing the maximum depth, the tree becomes simpler and is less likely to overfit.\n",
    "\n",
    "2. **Generalization**: Shallower trees are less likely to fit the training data perfectly, which means they are more likely to generalize well to new, unseen data. By constraining the depth, you encourage the tree to capture the most important features and patterns in the data rather than focusing on individual instances.\n",
    "\n",
    "3. **Reduced Variance**: Shallower trees have less variance, meaning they are less sensitive to fluctuations in the training data. This helps in creating a more stable and reliable model that performs consistently across different datasets.\n",
    "\n",
    "4. **Easier Interpretation**: Deeper trees can be difficult to interpret and visualize, while shallower trees are easier to understand. If you reduce the depth, the resulting tree's structure becomes simpler, making it easier to explain its decision-making process to stakeholders.\n",
    "\n",
    "However, it's important to strike a balance. If you reduce the maximum depth too much, the model might suffer from high bias, leading to underfitting. Underfitting occurs when the model is too simple to capture the complexities of the data. Therefore, it's advisable to use techniques like cross-validation to find an appropriate depth that minimizes overfitting without sacrificing too much on the model's ability to capture important patterns in the data.\n",
    "\n",
    "In summary, reducing the maximum depth of a Decision Tree can be an effective strategy to combat overfitting and improve the model's generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Explain if its a  good idea to try scaling the input features if a Decision Tree underfits the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS:Scaling the input features is generally not necessary and might not have a significant impact on addressing underfitting in a Decision Tree.\n",
    "\n",
    "Decision Trees are not influenced by the scale of the input features. They make binary decisions based on the values of individual features, and the splitting process is independent of the scale. Therefore, increasing or decreasing the scale of the features won't inherently affect the tree's ability to capture relationships between features and the target variable.\n",
    "\n",
    "Underfitting in Decision Trees is often a result of the tree being too shallow and simple, unable to capture the underlying patterns in the data. Addressing underfitting is more about adjusting the model's complexity rather than scaling the input features. Some approaches to consider for addressing underfitting in Decision Trees include:\n",
    "\n",
    "1. **Increasing Max Depth**: Allowing the tree to grow deeper can help it capture more complex relationships in the data. However, be cautious not to overdo it, as excessively deep trees can lead to overfitting.\n",
    "\n",
    "2. **Adding More Features**: If you have additional features that might contain relevant information, adding them to the model could help improve its ability to capture the underlying patterns.\n",
    "\n",
    "3. **Ensemble Methods**: Using ensemble methods like Random Forests or Gradient Boosting can combine multiple decision trees to create a more powerful and accurate model.\n",
    "\n",
    "4. **Feature Engineering**: Consider transforming or creating new features that might better represent the relationships within the data.\n",
    "\n",
    "In summary, scaling input features is unlikely to have a significant impact on addressing underfitting in Decision Trees. Instead, focus on adjusting the model's complexity and exploring other strategies to improve its performance on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. How much time will it take to train another Decision Tree on a training set of 10 million instances if it takes an hour to train a Decision Tree on a training set with 1 million instances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS:The time it takes to train a Decision Tree is not directly proportional to the number of instances in the training set due to various factors, such as the algorithm's complexity and the available computational resources. However, we can make a rough estimate based on the assumption that the time scales linearly with the number of instances.\n",
    "\n",
    "If it takes 1 hour to train a Decision Tree on a training set with 1 million instances, then to train a Decision Tree on a training set with 10 million instances, it might take around 10 hours, assuming that the complexity of the tree and the available computing resources remain the same.\n",
    "\n",
    "Please note that this is a simplified estimate and the actual time could vary based on factors like the specific algorithm used, the hardware setup, the data characteristics, and any optimization techniques applied during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Will setting presort=True speed up training if your training set has 100,000 instances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS:Setting `presort=True` in the context of training Decision Trees might not necessarily speed up training for a training set with 100,000 instances, and in fact, it might even slow down the training process in many cases.\n",
    "\n",
    "When `presort` is set to `True`, the Decision Tree algorithm pre-sorts the data for each feature before evaluating potential splits. This can help improve the efficiency of finding the best splits for each node, especially for small datasets where the overhead of sorting is not significant. However, when the dataset becomes larger, the sorting process can become computationally expensive and actually slow down the training process.\n",
    "\n",
    "For larger datasets like one with 100,000 instances, the overhead of sorting the data for each feature can become quite substantial, and the time spent on sorting might outweigh the benefits of more efficient split evaluation. As a result, setting `presort=True` might lead to longer training times.\n",
    "\n",
    "In practice, it's often recommended to leave `presort` set to its default value of `False` for larger datasets. The Decision Tree algorithm usually employs heuristics to determine whether to use presorting or not based on the size of the dataset. For smaller datasets, it might make sense to use `presort=True` to potentially speed up training, but for larger datasets like the one with 100,000 instances, it's generally better to rely on the algorithm's default behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Follow these steps to train and fine-tune a Decision Tree for the moons dataset:\n",
    "\n",
    "* To build a moons dataset, use make moons(n samples=10000, noise=0.4).\n",
    "\n",
    "* Divide the dataset into a training and a test collection with train test split().\n",
    "\n",
    "* To find good hyperparameters values for a DecisionTreeClassifier, use grid search with cross-validation (with the GridSearchCV class). Try different values for max leaf nodes.\n",
    "\n",
    "* Use these hyperparameters to train the model on the entire training set, and then assess its output on the test set. You can achieve an accuracy of 85 to 87 percent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.datasets import make_circles, make_classification, make_moons\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=make_moons(n_samples=10000,noise=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 2) (2000, 2)\n",
      "(8000,) (2000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,X_test.shape)\n",
    "print(y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=dt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy 0.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_leaf_nodes': 40}\n",
      "Test Set Accuracy: 0.8765\n"
     ]
    }
   ],
   "source": [
    "# lets predict output\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred\n",
    "\n",
    "# Accuracy without Huperparameter tunning \n",
    "print('test_accuracy',accuracy_score(y_pred,y_test))\n",
    "# Step 3: Perform grid search for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'max_leaf_nodes': [None, 10, 20, 30, 40]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "# Step 4: Train the model with the best hyperparameters on the entire training set\n",
    "best_tree = DecisionTreeClassifier(**best_params, random_state=42)\n",
    "best_tree.fit(X_train, y_train)\n",
    "# Step 5: Assess the model's performance on the test set\n",
    "y_pred = best_tree.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Set Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is clearly shown with hyperparameter tuning accuracy on test set is increased from 81% to 87%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Follow these steps to grow a forest:\n",
    "\n",
    "* Using the same method as before, create 1,000 subsets of the training set, each containing 100 instances chosen at random. You can do this with Scikit-ShuffleSplit Learn's class.\n",
    "\n",
    "* Using the best hyperparameter values found in the previous exercise, train one Decision Tree on each subset. On the test collection, evaluate these 1,000 Decision Trees. These Decision        Trees would likely perform worse than the first Decision Tree, achieving only around 80% accuracy, since they were trained on smaller sets.\n",
    "\n",
    "* Now the magic begins. Create 1,000 Decision Tree predictions for each test set case, and keep only the most common prediction (you can do this with SciPy's mode() function). Over the test collection, this method gives you majority-vote predictions.\n",
    "\n",
    "* d. On the test range, evaluate these predictions: you should achieve a slightly higher accuracy than the first model (approx 0.5 to 1.5 percent higher). You've successfully learned a Random Forest classifier!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.datasets import make_circles, make_classification, make_moons\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=make_moons(n_samples=10000,random_state=42,noise=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of ShuffleSplit\n",
    "n_splits = 1000  # Number of subsets\n",
    "subset_size = 100  # Number of instances in each subset\n",
    "shuffle_split = ShuffleSplit(n_splits=n_splits, test_size=subset_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1,000 subsets of the training set\n",
    "subsets = []\n",
    "for train_index, _ in shuffle_split.split(X):\n",
    "    subset = X[train_index]\n",
    "    subsets.append(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train Decision Trees on each subset and evaluate on the test set\n",
    "best_params = {'max_leaf_nodes': 20}  # Use the best hyperparameters from previous exercise\n",
    "forest_predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subset in subsets:\n",
    "    tree_classifier = DecisionTreeClassifier(**best_params, random_state=42)\n",
    "    tree_classifier.fit(subset, y[train_index])\n",
    "    tree_predictions = tree_classifier.predict(X_test)\n",
    "    forest_predictions.append(tree_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "# Step 4: Perform majority-vote predictions using mode\n",
    "forest_predictions = np.array(forest_predictions)\n",
    "majority_vote_predictions, _ = mode(forest_predictions, axis=0)\n",
    "# Convert majority_vote_predictions to a single prediction array based on majority voting\n",
    "forest_predictions = mode(majority_vote_predictions, axis=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.491\n"
     ]
    }
   ],
   "source": [
    "# Convert majority_vote_predictions to a single prediction array based on majority voting\n",
    "forest_predictions = majority_vote_predictions\n",
    "\n",
    "# Evaluate the Random Forest accuracy on the test set\n",
    "forest_accuracy = accuracy_score(y_test, forest_predictions)\n",
    "print(\"Random Forest Accuracy:\", forest_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Decision Tree Accuracy: 0.498338500000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Generate the moons dataset\n",
    "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\n",
    "\n",
    "# Assuming you have split your data into y_train and y_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Create subsets of the training set using ShuffleSplit\n",
    "n_splits = 1000  # Number of subsets\n",
    "subset_size = 100  # Number of instances in each subset\n",
    "shuffle_split = ShuffleSplit(n_splits=n_splits, test_size=subset_size, random_state=42)\n",
    "\n",
    "# Create 1,000 subsets of the training set\n",
    "subsets = []\n",
    "for train_index, _ in shuffle_split.split(X_train):\n",
    "    subset = X_train[train_index]\n",
    "    subsets.append(subset)\n",
    "\n",
    "# Step 3: Train Decision Trees on each subset and evaluate on the test set\n",
    "best_params = {'max_leaf_nodes': 20}  # Use the best hyperparameters from previous exercise\n",
    "\n",
    "# Step 3: Train Decision Trees on each subset and evaluate on the test collection\n",
    "forest_accuracies = []\n",
    "\n",
    "for subset_X in subsets:\n",
    "    subset_indices = train_index[:subset_X.shape[0]]  # Extract corresponding indices\n",
    "    subset_y = y_train[subset_indices]  # Extract corresponding labels\n",
    "    tree_classifier = DecisionTreeClassifier(**best_params, random_state=42)\n",
    "    tree_classifier.fit(subset_X, subset_y)\n",
    "    tree_predictions = tree_classifier.predict(X_test)\n",
    "    forest_accuracies.append(accuracy_score(y_test, tree_predictions))\n",
    "\n",
    "# Calculate the average accuracy of the 1,000 Decision Trees\n",
    "average_accuracy = sum(forest_accuracies) / len(forest_accuracies)\n",
    "print(\"Average Decision Tree Accuracy:\", average_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Accuracy: 0.869\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Step 1: Generate the moons dataset\n",
    "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\n",
    "\n",
    "# Assuming you have split your data into y_train and y_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Create 1000 subsets using ShuffleSplit\n",
    "n_subsets = 1000\n",
    "subset_size = 100\n",
    "shuffler = ShuffleSplit(n_splits=n_subsets, test_size=subset_size, random_state=42)\n",
    "subsets_indices = shuffler.split(X_train)\n",
    "\n",
    "# Initialize an array to store the trained decision trees\n",
    "trained_trees = []\n",
    "\n",
    "# Step 3: Train Decision Trees on subsets and store them\n",
    "for train_index, _ in subsets_indices:\n",
    "    subset_X = X_train[train_index]\n",
    "    subset_y = y_train[train_index]\n",
    "    \n",
    "    tree = DecisionTreeClassifier(max_leaf_nodes=20)\n",
    "    tree.fit(subset_X, subset_y)\n",
    "    trained_trees.append(tree)\n",
    "\n",
    "# Step 4: Make predictions using the trained trees\n",
    "def predict_majority_vote(trees, X):\n",
    "    predictions = np.array([tree.predict(X) for tree in trees])\n",
    "    majority_predictions, _ = mode(predictions)\n",
    "    return majority_predictions\n",
    "\n",
    "# Step 5: Evaluate the ensemble's predictions on the test set\n",
    "ensemble_predictions = predict_majority_vote(trained_trees, X_test)\n",
    "accuracy = np.mean(ensemble_predictions == y_test)\n",
    "\n",
    "print(\"Ensemble Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
